{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.utils as sku\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lock random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# File path for noise descriptor\n",
    "noise_file = \"../Python/noise_description.json\"\n",
    "\n",
    "# Configuration for training data\n",
    "training_samples = 1200\n",
    "validation_samples = 1200\n",
    "training_file = \"../TrainingData/neodata/all_setpoints_500.csv\"\n",
    "\n",
    "# Configuration for test data\n",
    "test_samples = 100\n",
    "test_file = \"../TestData/neodata/all_setpoints_500.csv\"\n",
    "\n",
    "# Features to pop from all data sets\n",
    "features_for_removal = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Add noise and sample training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training data\n",
    "# Loop through all files\n",
    "\n",
    "# Lock random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load data and noise description\n",
    "all_data = pd.read_csv(training_file)\n",
    "with open(noise_file) as file: noise_desc = json.load(file)\n",
    "\n",
    "# For each noise descriptor, add noise accordingly\n",
    "for feature in noise_desc:\n",
    "    if noise_desc[feature]['var'] > 0:\n",
    "        noise = np.random.normal(noise_desc[feature]['mean'],noise_desc[feature]['var'],(len(all_data)))\n",
    "        all_data[feature] += noise\n",
    "\n",
    "# Remove unwanted features\n",
    "for feature in features_for_removal:\n",
    "    all_data.pop(feature)\n",
    "\n",
    "for_export_train = None\n",
    "for_export_valid = None\n",
    "for f in range(21): # Loop over classes\n",
    "    \n",
    "    # Extract >> training_samples + validation_samples << randomly from class data\n",
    "    fault_data = sku.resample(all_data[all_data['target'] == f],replace=False,n_samples=training_samples+validation_samples, random_state = 42)\n",
    "    \n",
    "    # Set first >>training_samples<< number of samples for training data \n",
    "    for_export_train = pd.concat([for_export_train,fault_data[0:training_samples]])\n",
    "    \n",
    "    # Set next >>validation_samples<< number of samples for validation data\n",
    "    for_export_valid = pd.concat([for_export_valid,fault_data[training_samples:training_samples+validation_samples]])\n",
    "\n",
    "# Export training data\n",
    "for_export_train.to_csv(f\"./../TrainingData/neodata/14d_setpoints_{training_samples}.csv\", index=None,header=True)\n",
    "\n",
    "# Export training data\n",
    "for_export_valid.to_csv(f\"./../ValidationData/neodata/14d_setpoints_{validation_samples}.csv\", index=None,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Add noise and sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training data\n",
    "# Loop through all files\n",
    "\n",
    "# Lock random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load data and noise description\n",
    "all_data = pd.read_csv(test_file)\n",
    "with open(noise_file) as file: noise_desc = json.load(file)\n",
    "\n",
    "# For each noise descriptor, add noise accordingly\n",
    "for feature in noise_desc:\n",
    "    if noise_desc[feature]['var'] > 0:\n",
    "        noise = np.random.normal(noise_desc[feature]['mean'],noise_desc[feature]['var'],(len(all_data)))\n",
    "        all_data[feature] += noise\n",
    "\n",
    "for feature in features_for_removal:\n",
    "    all_data.pop(feature)\n",
    "\n",
    "# Sample noisy data and concatenate it\n",
    "for_export_test = None\n",
    "for f in range(21): # Loop over classes\n",
    "    for_export_test = pd.concat([for_export_test,sku.resample(all_data[all_data['target'] == f],replace=False,n_samples=test_samples, random_state = 42)])\n",
    "\n",
    "# Export training data\n",
    "for_export_test.to_csv(f\"./../TestData/neodata/14d_setpoints_{test_samples}.csv\", index=None,header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3f2a733c2768fd0b48f567c9c49c153ac06dfa727864d4e3c9ae02200777f8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
